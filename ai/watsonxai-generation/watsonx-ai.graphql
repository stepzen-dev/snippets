extend type Query {
  """
  Internal helper that accepts a Secret token and returns it as JSON.
  This allows the Secret type to be converted to JSON for injection.
  """
  _token_to_json(token: Secret!): JSON
    @value(
      script: {
        src: """
        {"token": token }
        """
        language: JSONATA
      }
    )

  """
  Extract IBM IAM token and inject it into wxai_generation for authentication.
  This makes the access_token available as an expansion variable.
  """
  _inject_ibm_iam_token: JSON
    @inject(on: [{ expose: true, types: "Query", fields: "wxai_generation" }])
    @sequence(
      steps: [
        { query: "ibm_iam_token" }
        { query: "_token_to_json", arguments: [{ name: "token", field: "ยง0" }] }
      ]
    )
  """
  Infers the next tokens for a given deployed model from the `input` text.
  """
  wxai_generation(
    input: String
    model_id: String = "ibm/granite-3-8b-instruct"
    parameters: WXAI_GenerationParameters
  ): WXAI_GenerationResponse
    @rest(
      method: POST
      endpoint: "$url;"
      path: "/ml/v1/text/generation?version=$version"
      headers: [{ name: "authorization", value: "Bearer $token" }]
      ecmascript: """
      function bodyPOST(s) {
          let body = JSON.parse(s)
          body.project_id = get("project_id")
          return JSON.stringify(body)
      }
      """
      configuration: "watsonx-service"
    )
}

"""
The parameters for the model of a generative AI request. See https://cloud.ibm.com/apidocs/watsonx-ai#text-generation for more details.
"""
input WXAI_GenerationParameters {
  """
  The temperature specifies the amount of variation in the generation process when using sampling mode.
  """
  temperature: Float = 0
  """
  The strategy the model uses to choose the tokens in the generated output.
  """
  decoding_method: String
  """
  The minimum number of new tokens to be generated.
  """
  min_new_tokens: Int
  """
  The maximum number of new tokens to be generated.
  """
  max_new_tokens: Int = 20
  """
  The repetition penalty lowers the probability scores of tokens that have already been generated or belong to the context.
  """
  repetition_penalty: Float
  """
  A string of one or more characters which will stop the text generation if/when they appear in the generated output.
  """
  stop_sequences: [String]
}

"""
Response of IBM watsonx.ai `generation` endpoint
"""
type WXAI_GenerationResponse {
  created_at: DateTime
  model_id: String
  model_version: String
  results: [WXAI_GenerationResult]
}

"""
A result in the response of IBM watsonx.ai `generation` endpoint
"""
type WXAI_GenerationResult {
  generated_text: String
  generated_token_count: Int
  input_token_count: Int
  stop_reason: String
}
